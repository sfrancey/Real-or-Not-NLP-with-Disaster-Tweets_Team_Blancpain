{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Updated_ New_Notebook_Blancpain.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sfrancey/Real-or-Not-NLP-with-Disaster-Tweets_Team_Blancpain/blob/main/Code/Updated__New_Notebook_Blancpain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEA9bJP9nXuD"
      },
      "source": [
        "#DMML TEAM BLANCPAIN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_joXxs5cncOK"
      },
      "source": [
        "##Import some librairies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLs2TYMXnkrn"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import collections  as mc\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import pandas as pd \n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "sns.set_style(\"white\")\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import datasets\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "from sklearn.base import TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "import string\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from spacy.lang.en import English\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from sklearn.base import TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "import spacy\n",
        "from gensim.models import Word2Vec\n",
        "pip install autocorrect\n",
        "from autocorrect import Speller\n",
        "spell = Speller(lang='en')\n",
        "\n",
        "%matplotlib inline\n",
        "sns.set_style(\"dark\")\n",
        "\n",
        "# Hide warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqF5jThxnsXJ"
      },
      "source": [
        "##Data Importation##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BM5SUhspoHyz"
      },
      "source": [
        "df_train = pd.read_csv('https://raw.githubusercontent.com/sfrancey/Real-or-Not-NLP-with-Disaster-Tweets_Team_Blancpain/main/Data/training_data.csv')\n",
        "\n",
        "df_test = pd.read_csv('https://raw.githubusercontent.com/sfrancey/Real-or-Not-NLP-with-Disaster-Tweets_Team_Blancpain/main/Data/test_data.csv')\n",
        "\n",
        "df_sample = pd.read_csv('https://raw.githubusercontent.com/sfrancey/Real-or-Not-NLP-with-Disaster-Tweets_Team_Blancpain/main/Data/sample_submission.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYUyrcLcoeC9"
      },
      "source": [
        "##Data visualization##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvR6D2f3or1z"
      },
      "source": [
        "###Training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "RJ4MVIQAok60",
        "outputId": "61de1b25-4690-48d9-f687-401bd774f356"
      },
      "source": [
        "df_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3738</td>\n",
              "      <td>destroyed</td>\n",
              "      <td>USA</td>\n",
              "      <td>Black Eye 9: A space battle occurred at Star O...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>853</td>\n",
              "      <td>bioterror</td>\n",
              "      <td>NaN</td>\n",
              "      <td>#world FedEx no longer to transport bioterror ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10540</td>\n",
              "      <td>windstorm</td>\n",
              "      <td>Palm Beach County, FL</td>\n",
              "      <td>Reality Training: Train falls off elevated tra...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5988</td>\n",
              "      <td>hazardous</td>\n",
              "      <td>USA</td>\n",
              "      <td>#Taiwan Grace: expect that large rocks trees m...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6328</td>\n",
              "      <td>hostage</td>\n",
              "      <td>Australia</td>\n",
              "      <td>New ISIS Video: ISIS Threatens to Behead Croat...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6466</th>\n",
              "      <td>4377</td>\n",
              "      <td>earthquake</td>\n",
              "      <td>ARGENTINA</td>\n",
              "      <td>#Earthquake #Sismo M 1.9 - 15km E of Anchorage...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6467</th>\n",
              "      <td>3408</td>\n",
              "      <td>derail</td>\n",
              "      <td>NaN</td>\n",
              "      <td>@EmiiliexIrwin Totally agree.She is 23 and kno...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6468</th>\n",
              "      <td>9794</td>\n",
              "      <td>trapped</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Hollywood Movie About Trapped Miners Released ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6469</th>\n",
              "      <td>10344</td>\n",
              "      <td>weapons</td>\n",
              "      <td>Beirut/Toronto</td>\n",
              "      <td>Friendly reminder that the only country to eve...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6470</th>\n",
              "      <td>1779</td>\n",
              "      <td>buildings%20on%20fire</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Buildings are on fire and they have time for a...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6471 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         id  ... target\n",
              "0      3738  ...      0\n",
              "1       853  ...      0\n",
              "2     10540  ...      1\n",
              "3      5988  ...      1\n",
              "4      6328  ...      1\n",
              "...     ...  ...    ...\n",
              "6466   4377  ...      1\n",
              "6467   3408  ...      0\n",
              "6468   9794  ...      1\n",
              "6469  10344  ...      1\n",
              "6470   1779  ...      1\n",
              "\n",
              "[6471 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qURApsPqoncX",
        "outputId": "4c08c738-6203-486e-9efe-c8de4406b23a"
      },
      "source": [
        "print(\"There are {0} rows and {1} columns in the train dataset.\".format(df_train.shape[0],df_train.shape[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 6471 rows and 5 columns in the train dataset.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2OXEAZ_ou10"
      },
      "source": [
        "###Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "1MchgznVoqL1",
        "outputId": "664dabdb-ddba-4c96-f068-2ab75fcc5fb9"
      },
      "source": [
        "df_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9972</td>\n",
              "      <td>tsunami</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Crptotech tsunami and banks.\\n http://t.co/KHz...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9865</td>\n",
              "      <td>traumatised</td>\n",
              "      <td>Portsmouth, UK</td>\n",
              "      <td>I'm that traumatised that I can't even spell p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1937</td>\n",
              "      <td>burning%20buildings</td>\n",
              "      <td>NaN</td>\n",
              "      <td>@foxnewsvideo @AIIAmericanGirI @ANHQDC So ... ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3560</td>\n",
              "      <td>desolate</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Me watching Law &amp;amp; Order (IB: @sauldale305)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2731</td>\n",
              "      <td>crushed</td>\n",
              "      <td>bahstun/porta reeko</td>\n",
              "      <td>Papi absolutely crushed that ball</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1137</th>\n",
              "      <td>3497</td>\n",
              "      <td>derailed</td>\n",
              "      <td>NaN</td>\n",
              "      <td>@ItsQueenBaby I'm at work it's a bunch of ppl ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1138</th>\n",
              "      <td>9191</td>\n",
              "      <td>suicide%20bomber</td>\n",
              "      <td>NaN</td>\n",
              "      <td>#?? #?? #??? #??? Suicide bomber kills 15 in S...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1139</th>\n",
              "      <td>10217</td>\n",
              "      <td>volcano</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Eruption of Indonesian volcano sparks transpor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1140</th>\n",
              "      <td>5308</td>\n",
              "      <td>fear</td>\n",
              "      <td>New York</td>\n",
              "      <td>Never let fear get in the way of achieving you...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1141</th>\n",
              "      <td>8044</td>\n",
              "      <td>refugees</td>\n",
              "      <td>NaN</td>\n",
              "      <td>wowo--=== 12000 Nigerian refugees repatriated ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1142 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         id  ...                                               text\n",
              "0      9972  ...  Crptotech tsunami and banks.\\n http://t.co/KHz...\n",
              "1      9865  ...  I'm that traumatised that I can't even spell p...\n",
              "2      1937  ...  @foxnewsvideo @AIIAmericanGirI @ANHQDC So ... ...\n",
              "3      3560  ...  Me watching Law &amp; Order (IB: @sauldale305)...\n",
              "4      2731  ...                  Papi absolutely crushed that ball\n",
              "...     ...  ...                                                ...\n",
              "1137   3497  ...  @ItsQueenBaby I'm at work it's a bunch of ppl ...\n",
              "1138   9191  ...  #?? #?? #??? #??? Suicide bomber kills 15 in S...\n",
              "1139  10217  ...  Eruption of Indonesian volcano sparks transpor...\n",
              "1140   5308  ...  Never let fear get in the way of achieving you...\n",
              "1141   8044  ...  wowo--=== 12000 Nigerian refugees repatriated ...\n",
              "\n",
              "[1142 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LxZk2puo9qt",
        "outputId": "990402a8-acec-4aa7-f703-769e0eca016d"
      },
      "source": [
        "print(\"There are {0} rows and {1} columns in the test dataset.\".format(df_test.shape[0],df_test.shape[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1142 rows and 4 columns in the test dataset.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "g1S1HLPzp6qU",
        "outputId": "4b7c3b0d-ede7-40d8-ef44-0576de787be5"
      },
      "source": [
        "df_sample.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   target\n",
              "0       0\n",
              "1       0\n",
              "2       0\n",
              "3       0\n",
              "4       0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OjBhP9fEHh-"
      },
      "source": [
        "###Drop Null values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMdir8zQELnj"
      },
      "source": [
        "df_train.fillna(\"Unknown\", inplace = True)\n",
        "df_test.fillna(\"Unknown\", inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPH63UivpBx7"
      },
      "source": [
        "###Base Rate "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxTO-v2jpKxb",
        "outputId": "a6a3e924-a228-42c3-cc5b-b7a614574c86"
      },
      "source": [
        "base_rate = max(len(df_train[df_train[\"target\"] == 0]) / len(df_train), len(df_train[df_train[\"target\"] == 1]) / len(df_train))\n",
        "print(\"The base rate for this problem is :\", base_rate)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The base rate for this problem is : 0.5719363313243703\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Fvm4vyypZ4f"
      },
      "source": [
        "##Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0nCT-mlodiP"
      },
      "source": [
        "#import emojis library\n",
        "!pip install emot\n",
        "import re\n",
        "from emot.emo_unicode import UNICODE_EMO, EMOTICONS\n",
        "\n",
        "#removing emojis function (emoticons displayed in text)\n",
        "def convert_emoticons_text(dataFrame):\n",
        "  len = dataFrame.shape[0]\n",
        "  i=0\n",
        "  while i<len:\n",
        "    for emot in EMOTICONS:\n",
        "        dataFrame.iloc[i] = re.sub(u'('+emot+')', \"_\".join(EMOTICONS[emot].replace(\",\",\"\").split()), dataFrame.iloc[i])\n",
        "    i+=1\n",
        "  return dataFrame.head(20)\n",
        "\n",
        "#removing emojis function (emoticons displayed in emojis)\n",
        "def convert_emoticons_emoji(dataFrame):\n",
        "  len = dataFrame.shape[0]\n",
        "  i=0\n",
        "  while i<len:\n",
        "    for emot in UNICODE_EMO:\n",
        "        dataFrame.iloc[i] = dataFrame.iloc[i].replace(emot, \"_\".join(UNICODE_EMO[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
        "    i+=1\n",
        "  return dataFrame.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uffo6ZMToxt8"
      },
      "source": [
        "#clean smileys in text\n",
        "#convert_emoticons(df_train['text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0L8vhbjp9oZ"
      },
      "source": [
        "df_train[\"keyword_text\"] = df_train['keyword'] + ' ' + df_train['text'] \n",
        "df_train[\"keyword_text\"] = df_train[\"keyword_text\"].astype(str)\n",
        "df_test[\"keyword_text\"] = df_test['keyword'] + ' ' + df_test['text'] \n",
        "df_test[\"keyword_text\"] = df_test[\"keyword_text\"].astype(str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDVNM8vIOCov"
      },
      "source": [
        "keyword_list = ['panicking',\n",
        " 'suicide%20bombing',\n",
        " 'body%20bags',\n",
        " 'blight',\n",
        " 'armageddon',\n",
        " 'stretcher',\n",
        " 'razed',\n",
        " 'ruin',\n",
        " 'wreckage',\n",
        " 'derailment',\n",
        " 'bombing',\n",
        " 'nuclear%20disaster',\n",
        " 'panic',\n",
        " 'obliteration',\n",
        " 'blazing',\n",
        " 'typhoon',\n",
        " 'outbreak',\n",
        " 'wrecked',\n",
        " 'oil%20spill',\n",
        " 'rescuers',\n",
        " 'debris',\n",
        " 'wildfire',\n",
        " 'aftershock',\n",
        " 'meltdown',\n",
        " 'bloody',\n",
        " 'traumatised',\n",
        " 'electrocute',\n",
        " 'smoke',\n",
        " 'screaming',\n",
        " 'body%20bag',\n",
        " 'suicide%20bomber',\n",
        " 'blew%20up']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtY94XjKOx8Q"
      },
      "source": [
        "def clean_keword_text(row):\n",
        "  if row['keyword'] in keyword_list:\n",
        "    return row['keyword'] + ' ' + row['text']\n",
        "  else:\n",
        "    return row['text']\n",
        "\n",
        "df_train[\"keyword_cleaned_text\"] = df_train.apply(clean_keword_text, axis=1)\n",
        "df_test[\"keyword_cleaned_text\"] = df_test.apply(clean_keword_text, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "-4JtEGnsp_4v",
        "outputId": "df1dc53b-9fd4-4b6e-f37e-f6e1a655cd62"
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "      <th>keyword_text</th>\n",
              "      <th>keyword_cleaned_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3738</td>\n",
              "      <td>destroyed</td>\n",
              "      <td>USA</td>\n",
              "      <td>Black Eye 9: A space battle occurred at Star O...</td>\n",
              "      <td>0</td>\n",
              "      <td>destroyed Black Eye 9: A space battle occurred...</td>\n",
              "      <td>Black Eye 9: A space battle occurred at Star O...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>853</td>\n",
              "      <td>bioterror</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>#world FedEx no longer to transport bioterror ...</td>\n",
              "      <td>0</td>\n",
              "      <td>bioterror #world FedEx no longer to transport ...</td>\n",
              "      <td>#world FedEx no longer to transport bioterror ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10540</td>\n",
              "      <td>windstorm</td>\n",
              "      <td>Palm Beach County, FL</td>\n",
              "      <td>Reality Training: Train falls off elevated tra...</td>\n",
              "      <td>1</td>\n",
              "      <td>windstorm Reality Training: Train falls off el...</td>\n",
              "      <td>Reality Training: Train falls off elevated tra...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5988</td>\n",
              "      <td>hazardous</td>\n",
              "      <td>USA</td>\n",
              "      <td>#Taiwan Grace: expect that large rocks trees m...</td>\n",
              "      <td>1</td>\n",
              "      <td>hazardous #Taiwan Grace: expect that large roc...</td>\n",
              "      <td>#Taiwan Grace: expect that large rocks trees m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6328</td>\n",
              "      <td>hostage</td>\n",
              "      <td>Australia</td>\n",
              "      <td>New ISIS Video: ISIS Threatens to Behead Croat...</td>\n",
              "      <td>1</td>\n",
              "      <td>hostage New ISIS Video: ISIS Threatens to Behe...</td>\n",
              "      <td>New ISIS Video: ISIS Threatens to Behead Croat...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id  ...                               keyword_cleaned_text\n",
              "0   3738  ...  Black Eye 9: A space battle occurred at Star O...\n",
              "1    853  ...  #world FedEx no longer to transport bioterror ...\n",
              "2  10540  ...  Reality Training: Train falls off elevated tra...\n",
              "3   5988  ...  #Taiwan Grace: expect that large rocks trees m...\n",
              "4   6328  ...  New ISIS Video: ISIS Threatens to Behead Croat...\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4SRuHS6n9gT"
      },
      "source": [
        "# Processing function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gymJUYqnMnv"
      },
      "source": [
        "# Load English language model\n",
        "sp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Create a list of punctuation marks\n",
        "punctuations = string.punctuation\n",
        "\n",
        "# Create a list of stopwords\n",
        "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
        "\n",
        "# Create a list of isolated letters\n",
        "lonely_letters = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\n",
        "\n",
        "# Create a list of further elements that could be deleted to improve the model\n",
        "to_be_deleted = ['http','html','@']\n",
        "\n",
        "def tokenize_function(text):\n",
        "    # Create token object, which is used to create documents with linguistic annotations.\n",
        "    sp_obj = sp(text)\n",
        "\n",
        "    # Lemmatize each token and convert each token into lowercase\n",
        "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in sp_obj ]\n",
        "\n",
        "    # Correcting spelling mistakes\n",
        "    #mytokens = [ spell(str(word)) for word in sp_obj]\n",
        "\n",
        "    ## Remove stop words\n",
        "    #mytokens = [ word for word in mytokens if word not in stop_words]\n",
        "\n",
        "    # Remove punctuation\n",
        "    #mytokens = [ word for word in mytokens if word not in punctuations]\n",
        "\n",
        "    # Remove isolated letters\n",
        "    #mytokens = [ word for word in mytokens if word not in lonely_letters]\n",
        "\n",
        "    # Delete further elements\n",
        "    #mytokens = [ word for word in mytokens if all(char not in word for char in to_be_deleted)]\n",
        "    \n",
        "    return mytokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8PALYuznOml",
        "outputId": "d46961be-8ba6-4f48-d134-130827e36b9f"
      },
      "source": [
        "tokenize_function(\"j'aime le machine learning788\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"j'aime\", 'le', 'machine', 'learning788']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqmp-bqVn6SW"
      },
      "source": [
        "# Create ngrams on text feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybrjHz9FnP_4"
      },
      "source": [
        "import gensim.downloader\n",
        "import multiprocessing\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "def eval_pipeline(df_train, df_test, tokenize_function, target_col='keyword', model_type='randomForest', feature_engineering='tfidf'):\n",
        "  ''' Eval pipeline '''\n",
        "\n",
        "  # Make copies to avoid any changes in the main datasets\n",
        "  train = df_train.copy()\n",
        "  test = df_test.copy()\n",
        "\n",
        "  # Define the model\n",
        "  if model_type == 'logisticRegression':\n",
        "    model = LogisticRegression(solver='lbfgs', #LogisticRegressionCV\n",
        "                                #cv=2, \n",
        "                                max_iter=1000, \n",
        "                                random_state=50,\n",
        "                                n_jobs=multiprocessing.cpu_count())\n",
        "  \n",
        "  elif model_type == 'randomForest':\n",
        "    model = RandomForestClassifier(n_estimators=200,\n",
        "                                   max_depth=8,\n",
        "                                   random_state=50,\n",
        "                                   n_jobs=multiprocessing.cpu_count())\n",
        "\n",
        "  elif model_type == 'knn':\n",
        "    model = KNeighborsClassifier(random_state=50,\n",
        "                                 n_jobs=multiprocessing.cpu_count())\n",
        "\n",
        "  # Define embeddings and model\n",
        "  if feature_engineering == 'tfidf':\n",
        "    feature_vector = TfidfVectorizer(tokenizer=tokenize_function)\n",
        "    model = Pipeline([('vectorizer', feature_vector), \n",
        "                   ('classifier', model)])\n",
        "  \n",
        "  elif feature_engineering == 'w2v':\n",
        "\n",
        "    # Function that uses w2v embeddings and compute the mean value of all the of w2v vectors of all the words\n",
        "    def document_vector(doc):\n",
        "      \"\"\"Create document vectors by averaging word vectors. Remove out-of-vocabulary words.\"\"\"\n",
        "      tokens = tokenize_function(doc)\n",
        "      doc = [word for word in tokens if word in feature_vector.wv.vocab]\n",
        "      return np.mean(feature_vector[doc], axis=0)\n",
        "\n",
        "    # Create corpus for w2v\n",
        "    sentences = []\n",
        "    for text in [list(train[target_col]), list(test[target_col])]:\n",
        "      for string in text:\n",
        "        sentences.append(tokenize_function(string))\n",
        "\n",
        "    # Create w2v embeddings\n",
        "    feature_vector = gensim.models.Word2Vec(sentences, \n",
        "                                            size=100, \n",
        "                                            window=10, \n",
        "                                            min_count=1, \n",
        "                                            iter=20,\n",
        "                                            seed=50,\n",
        "                                            workers=multiprocessing.cpu_count()\n",
        "                                            )\n",
        "\n",
        "    train[target_col] = train[target_col].apply(document_vector)\n",
        "    test[target_col] = test[target_col].apply(document_vector)\n",
        "  \n",
        "  # Split dataset\n",
        "  df_train_eval, df_validation = train_test_split(train, test_size=0.2, random_state = 50)\n",
        "\n",
        "  y_train_eval = df_train_eval[[\"target\"]]\n",
        "  y_validation = df_validation[[\"target\"]]\n",
        "\n",
        "  X1_train_eval = list(df_train_eval[target_col])\n",
        "  X1_validation = list(df_validation[target_col])\n",
        "\n",
        "  # Fit data\n",
        "  model.fit(X1_train_eval, y_train_eval)\n",
        "\n",
        "  # Eval data\n",
        "  train_accuracy_score = accuracy_score(y_train_eval, model.predict(X1_train_eval))\n",
        "  print(f\"The training accuracy is : {train_accuracy_score}\")\n",
        "\n",
        "  validation_accuracy_score = accuracy_score(y_validation, model.predict(X1_validation))\n",
        "  print(f\"The validation accuracy is : {validation_accuracy_score}\")\n",
        "\n",
        "  # Retrain on the full dataset\n",
        "  y_train = train[[\"target\"]]\n",
        "  X1_train = list(train[target_col])\n",
        "  X1_test = list(test[target_col])\n",
        "\n",
        "  # Create submission file\n",
        "  model.fit(X1_train, y_train)\n",
        "  submission = pd.DataFrame(model.predict(X1_test), columns=[\"target\"])\n",
        "  submission.to_csv(f'submission_{target_col}_{feature_engineering}_model_{model_type}_test2.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joU8n6qdq7Xi"
      },
      "source": [
        "###With \"Keyword\" feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mF8QhZD6OhAk",
        "outputId": "55545b79-3b2f-401a-811f-c3154fada48c"
      },
      "source": [
        "eval_pipeline(df_train, df_test, tokenize_function, target_col='keyword', model_type='randomForest', feature_engineering='tfidf')\n",
        "eval_pipeline(df_train, df_test, tokenize_function, target_col='keyword', model_type='randomForest', feature_engineering='w2v')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The training accuracy is : 0.7436244204018547\n",
            "The validation accuracy is : 0.6988416988416989\n",
            "The training accuracy is : 0.7436244204018547\n",
            "The validation accuracy is : 0.6988416988416989\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MgssP7jpEEk"
      },
      "source": [
        "eval_pipeline(df_train, df_test, tokenize_function, target_col='keyword', model_type='logisticRegression', feature_engineering='tfidf')\n",
        "eval_pipeline(df_train, df_test, tokenize_function, target_col='keyword', model_type='logisticRegression', feature_engineering='w2v')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYFI4-yyb_90"
      },
      "source": [
        "###With \"Text\" Feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPH9eT80UI5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34ddca64-8f3d-452e-fcfc-7a1f5209f069"
      },
      "source": [
        "eval_pipeline(df_train, df_test, tokenize_function, target_col='text', model_type='randomForest', feature_engineering='tfidf')\n",
        "eval_pipeline(df_train, df_test, tokenize_function, target_col='text', model_type='randomForest', feature_engineering='w2v')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The training accuracy is : 0.6477975270479135\n",
            "The validation accuracy is : 0.6471042471042471\n",
            "The training accuracy is : 0.8427357032457496\n",
            "The validation accuracy is : 0.7637065637065638\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuanpUoapHZ9"
      },
      "source": [
        "eval_pipeline(df_train, df_test, tokenize_function, target_col='text', model_type='logisticRegression', feature_engineering='tfidf')\n",
        "eval_pipeline(df_train, df_test, tokenize_function, target_col='text', model_type='logisticRegression', feature_engineering='w2v')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8y1jbGygN5w"
      },
      "source": [
        "###With \"Text\" and \"Keyword\" features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pj6aSW9WUN2i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a5042ca-c39e-4e7b-97b0-e748eee5ef08"
      },
      "source": [
        "eval_pipeline(df_train, df_test, tokenize_function, target_col='keyword_text', model_type='randomForest', feature_engineering='tfidf')\n",
        "eval_pipeline(df_train, df_test, tokenize_function, target_col='keyword_text', model_type='randomForest', feature_engineering='w2v')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The training accuracy is : 0.6549459041731066\n",
            "The validation accuracy is : 0.6548262548262548\n",
            "The training accuracy is : 0.857225656877898\n",
            "The validation accuracy is : 0.7760617760617761\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-uNGBQ3pKhA"
      },
      "source": [
        "eval_pipeline(df_train, df_test, tokenize_function, target_col='keyword_text', model_type='logisticRegression', feature_engineering='tfidf')\n",
        "eval_pipeline(df_train, df_test, tokenize_function, target_col='keyword_text', model_type='logisticRegression', feature_engineering='w2v')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f_xz28sTUfy"
      },
      "source": [
        "# With \"Text\" and \"Keyword\" cleaned"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZxmd_XBKkPy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dfbd7de-4344-4823-9923-33b23801441c"
      },
      "source": [
        "eval_pipeline(df_train, df_test, tokenize_function, target_col='keyword_cleaned_text', model_type='randomForest', feature_engineering='tfidf')\n",
        "eval_pipeline(df_train, df_test, tokenize_function, target_col='keyword_cleaned_text', model_type='randomForest', feature_engineering='w2v')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The training accuracy is : 0.6505023183925811\n",
            "The validation accuracy is : 0.6501930501930502\n",
            "The training accuracy is : 0.8543276661514683\n",
            "The validation accuracy is : 0.7791505791505792\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAabJUGeee_z"
      },
      "source": [
        "eval_pipeline(df_train, df_test, tokenize_function, target_col='keyword_cleaned_text', model_type='logisticRegression', feature_engineering='tfidf')\n",
        "eval_pipeline(df_train, df_test, tokenize_function, target_col='keyword_cleaned_text', model_type='logisticRegression', feature_engineering='w2v')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}