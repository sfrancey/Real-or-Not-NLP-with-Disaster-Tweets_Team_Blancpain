{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Updated_ New_Notebook_Blancpain.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sfrancey/Real-or-Not-NLP-with-Disaster-Tweets_Team_Blancpain/blob/main/Updated__New_Notebook_Blancpain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEA9bJP9nXuD"
      },
      "source": [
        "#DMML TEAM BLANCPAIN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_joXxs5cncOK"
      },
      "source": [
        "##Import some librairies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLs2TYMXnkrn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1487b08d-9d68-4965-8c2a-8397e368c8c7"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import collections  as mc\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import pandas as pd \n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "sns.set_style(\"white\")\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import datasets\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "from sklearn.base import TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "import string\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from spacy.lang.en import English\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from sklearn.base import TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "import spacy\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "%matplotlib inline\n",
        "sns.set_style(\"dark\")\n",
        "\n",
        "# Hide warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqF5jThxnsXJ"
      },
      "source": [
        "##Data Importation##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BM5SUhspoHyz"
      },
      "source": [
        "df_train = pd.read_csv('https://raw.githubusercontent.com/sfrancey/Real-or-Not-NLP-with-Disaster-Tweets_Team_Blancpain/main/Data/training_data.csv')\n",
        "\n",
        "df_test = pd.read_csv('https://raw.githubusercontent.com/sfrancey/Real-or-Not-NLP-with-Disaster-Tweets_Team_Blancpain/main/Data/test_data.csv')\n",
        "\n",
        "df_sample = pd.read_csv('https://raw.githubusercontent.com/sfrancey/Real-or-Not-NLP-with-Disaster-Tweets_Team_Blancpain/main/Data/sample_submission.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYUyrcLcoeC9"
      },
      "source": [
        "##Data visualization##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvR6D2f3or1z"
      },
      "source": [
        "###Training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "RJ4MVIQAok60",
        "outputId": "9c261aae-a7c1-4020-d749-d78730767ff8"
      },
      "source": [
        "df_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3738</td>\n",
              "      <td>destroyed</td>\n",
              "      <td>USA</td>\n",
              "      <td>Black Eye 9: A space battle occurred at Star O...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>853</td>\n",
              "      <td>bioterror</td>\n",
              "      <td>NaN</td>\n",
              "      <td>#world FedEx no longer to transport bioterror ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10540</td>\n",
              "      <td>windstorm</td>\n",
              "      <td>Palm Beach County, FL</td>\n",
              "      <td>Reality Training: Train falls off elevated tra...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5988</td>\n",
              "      <td>hazardous</td>\n",
              "      <td>USA</td>\n",
              "      <td>#Taiwan Grace: expect that large rocks trees m...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6328</td>\n",
              "      <td>hostage</td>\n",
              "      <td>Australia</td>\n",
              "      <td>New ISIS Video: ISIS Threatens to Behead Croat...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6466</th>\n",
              "      <td>4377</td>\n",
              "      <td>earthquake</td>\n",
              "      <td>ARGENTINA</td>\n",
              "      <td>#Earthquake #Sismo M 1.9 - 15km E of Anchorage...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6467</th>\n",
              "      <td>3408</td>\n",
              "      <td>derail</td>\n",
              "      <td>NaN</td>\n",
              "      <td>@EmiiliexIrwin Totally agree.She is 23 and kno...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6468</th>\n",
              "      <td>9794</td>\n",
              "      <td>trapped</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Hollywood Movie About Trapped Miners Released ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6469</th>\n",
              "      <td>10344</td>\n",
              "      <td>weapons</td>\n",
              "      <td>Beirut/Toronto</td>\n",
              "      <td>Friendly reminder that the only country to eve...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6470</th>\n",
              "      <td>1779</td>\n",
              "      <td>buildings%20on%20fire</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Buildings are on fire and they have time for a...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6471 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         id  ... target\n",
              "0      3738  ...      0\n",
              "1       853  ...      0\n",
              "2     10540  ...      1\n",
              "3      5988  ...      1\n",
              "4      6328  ...      1\n",
              "...     ...  ...    ...\n",
              "6466   4377  ...      1\n",
              "6467   3408  ...      0\n",
              "6468   9794  ...      1\n",
              "6469  10344  ...      1\n",
              "6470   1779  ...      1\n",
              "\n",
              "[6471 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qURApsPqoncX",
        "outputId": "15cd11b8-406a-47bc-8583-f384681cc109"
      },
      "source": [
        "print(\"There are {0} rows and {1} columns in the train dataset.\".format(df_train.shape[0],df_train.shape[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 6471 rows and 5 columns in the train dataset.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2OXEAZ_ou10"
      },
      "source": [
        "###Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "1MchgznVoqL1",
        "outputId": "2f1c67dc-3210-4f45-9473-a89891e1a610"
      },
      "source": [
        "df_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9972</td>\n",
              "      <td>tsunami</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Crptotech tsunami and banks.\\n http://t.co/KHz...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9865</td>\n",
              "      <td>traumatised</td>\n",
              "      <td>Portsmouth, UK</td>\n",
              "      <td>I'm that traumatised that I can't even spell p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1937</td>\n",
              "      <td>burning%20buildings</td>\n",
              "      <td>NaN</td>\n",
              "      <td>@foxnewsvideo @AIIAmericanGirI @ANHQDC So ... ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3560</td>\n",
              "      <td>desolate</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Me watching Law &amp;amp; Order (IB: @sauldale305)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2731</td>\n",
              "      <td>crushed</td>\n",
              "      <td>bahstun/porta reeko</td>\n",
              "      <td>Papi absolutely crushed that ball</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1137</th>\n",
              "      <td>3497</td>\n",
              "      <td>derailed</td>\n",
              "      <td>NaN</td>\n",
              "      <td>@ItsQueenBaby I'm at work it's a bunch of ppl ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1138</th>\n",
              "      <td>9191</td>\n",
              "      <td>suicide%20bomber</td>\n",
              "      <td>NaN</td>\n",
              "      <td>#?? #?? #??? #??? Suicide bomber kills 15 in S...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1139</th>\n",
              "      <td>10217</td>\n",
              "      <td>volcano</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Eruption of Indonesian volcano sparks transpor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1140</th>\n",
              "      <td>5308</td>\n",
              "      <td>fear</td>\n",
              "      <td>New York</td>\n",
              "      <td>Never let fear get in the way of achieving you...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1141</th>\n",
              "      <td>8044</td>\n",
              "      <td>refugees</td>\n",
              "      <td>NaN</td>\n",
              "      <td>wowo--=== 12000 Nigerian refugees repatriated ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1142 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         id  ...                                               text\n",
              "0      9972  ...  Crptotech tsunami and banks.\\n http://t.co/KHz...\n",
              "1      9865  ...  I'm that traumatised that I can't even spell p...\n",
              "2      1937  ...  @foxnewsvideo @AIIAmericanGirI @ANHQDC So ... ...\n",
              "3      3560  ...  Me watching Law &amp; Order (IB: @sauldale305)...\n",
              "4      2731  ...                  Papi absolutely crushed that ball\n",
              "...     ...  ...                                                ...\n",
              "1137   3497  ...  @ItsQueenBaby I'm at work it's a bunch of ppl ...\n",
              "1138   9191  ...  #?? #?? #??? #??? Suicide bomber kills 15 in S...\n",
              "1139  10217  ...  Eruption of Indonesian volcano sparks transpor...\n",
              "1140   5308  ...  Never let fear get in the way of achieving you...\n",
              "1141   8044  ...  wowo--=== 12000 Nigerian refugees repatriated ...\n",
              "\n",
              "[1142 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LxZk2puo9qt",
        "outputId": "7d340545-ee7a-40f4-8630-0b24a93691f2"
      },
      "source": [
        "print(\"There are {0} rows and {1} columns in the test dataset.\".format(df_test.shape[0],df_test.shape[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1142 rows and 4 columns in the test dataset.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "g1S1HLPzp6qU",
        "outputId": "eb975982-9a8e-438f-d6ee-f88d1cd7e35b"
      },
      "source": [
        "df_sample.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   target\n",
              "0       0\n",
              "1       0\n",
              "2       0\n",
              "3       0\n",
              "4       0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OjBhP9fEHh-"
      },
      "source": [
        "###Drop Null values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMdir8zQELnj"
      },
      "source": [
        "df_train.fillna(\"Unknown\", inplace = True)\n",
        "df_test.fillna(\"Unknown\", inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPH63UivpBx7"
      },
      "source": [
        "###Base Rate "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxTO-v2jpKxb",
        "outputId": "b71a7900-d23d-4faf-da14-c6184e602b77"
      },
      "source": [
        "base_rate = max(len(df_train[df_train[\"target\"] == 0]) / len(df_train), len(df_train[df_train[\"target\"] == 1]) / len(df_train))\n",
        "print(\"The base rate for this problem is :\", base_rate)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The base rate for this problem is : 0.5719363313243703\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Fvm4vyypZ4f"
      },
      "source": [
        "##Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0L8vhbjp9oZ"
      },
      "source": [
        "df_train[\"keyword_text\"] = df_train['keyword'] + ' ' + df_train['text'] \n",
        "df_train[\"keyword_text\"] = df_train[\"keyword_text\"].astype(str)\n",
        "df_test[\"keyword_text\"] = df_test['keyword'] + ' ' + df_test['text'] \n",
        "df_test[\"keyword_text\"] = df_test[\"keyword_text\"].astype(str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "-4JtEGnsp_4v",
        "outputId": "62003f46-620a-40b6-b625-9464b165628b"
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "      <th>keyword_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3738</td>\n",
              "      <td>destroyed</td>\n",
              "      <td>USA</td>\n",
              "      <td>Black Eye 9: A space battle occurred at Star O...</td>\n",
              "      <td>0</td>\n",
              "      <td>destroyed Black Eye 9: A space battle occurred...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>853</td>\n",
              "      <td>bioterror</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>#world FedEx no longer to transport bioterror ...</td>\n",
              "      <td>0</td>\n",
              "      <td>bioterror #world FedEx no longer to transport ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10540</td>\n",
              "      <td>windstorm</td>\n",
              "      <td>Palm Beach County, FL</td>\n",
              "      <td>Reality Training: Train falls off elevated tra...</td>\n",
              "      <td>1</td>\n",
              "      <td>windstorm Reality Training: Train falls off el...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5988</td>\n",
              "      <td>hazardous</td>\n",
              "      <td>USA</td>\n",
              "      <td>#Taiwan Grace: expect that large rocks trees m...</td>\n",
              "      <td>1</td>\n",
              "      <td>hazardous #Taiwan Grace: expect that large roc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6328</td>\n",
              "      <td>hostage</td>\n",
              "      <td>Australia</td>\n",
              "      <td>New ISIS Video: ISIS Threatens to Behead Croat...</td>\n",
              "      <td>1</td>\n",
              "      <td>hostage New ISIS Video: ISIS Threatens to Behe...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id    keyword  ... target                                       keyword_text\n",
              "0   3738  destroyed  ...      0  destroyed Black Eye 9: A space battle occurred...\n",
              "1    853  bioterror  ...      0  bioterror #world FedEx no longer to transport ...\n",
              "2  10540  windstorm  ...      1  windstorm Reality Training: Train falls off el...\n",
              "3   5988  hazardous  ...      1  hazardous #Taiwan Grace: expect that large roc...\n",
              "4   6328    hostage  ...      1  hostage New ISIS Video: ISIS Threatens to Behe...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4SRuHS6n9gT"
      },
      "source": [
        "# Processing function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gymJUYqnMnv"
      },
      "source": [
        "# Load English language model\n",
        "sp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Create a list of punctuation marks\n",
        "punctuations = string.punctuation\n",
        "\n",
        "# Create a list of stopwords\n",
        "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
        "\n",
        "def tokenize_function(text):\n",
        "    # Create token object, which is used to create documents with linguistic annotations.\n",
        "    sp_obj = sp(text)\n",
        "\n",
        "    # Lemmatize each token and convert each token into lowercase\n",
        "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in sp_obj ]\n",
        "\n",
        "    # Remove stop words and punctuation\n",
        "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
        "    \n",
        "    return mytokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8PALYuznOml",
        "outputId": "f03d44aa-f571-4d59-9c9d-b6ffab767698"
      },
      "source": [
        "tokenize_function(\"j'aime le machine learning788\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"j'aime\", 'le', 'machine', 'learning788']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqmp-bqVn6SW"
      },
      "source": [
        "# Create ngrams on text feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Db1EhWVln2Vg"
      },
      "source": [
        "from nltk.util import ngrams\n",
        "import collections\n",
        "\n",
        "corpus = []\n",
        "for text in [list(df_train['text']), list(df_test['text'])]:\n",
        "  for string in text:\n",
        "    corpus.extend(tokenize_function(string))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gShKJTrmtQ2L",
        "outputId": "99be5ce1-6a78-48aa-b87b-9ab0a2be405b"
      },
      "source": [
        "collections.Counter(ngrams(corpus, 1)).most_common(30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('...',), 944),\n",
              " (('like',), 394),\n",
              " (('fire',), 347),\n",
              " (('amp',), 298),\n",
              " (('new',), 226),\n",
              " (('people',), 198),\n",
              " (('news',), 191),\n",
              " (('kill',), 176),\n",
              " (('video',), 174),\n",
              " (('burn',), 167),\n",
              " (('2',), 167),\n",
              " (('crash',), 162),\n",
              " (('emergency',), 159),\n",
              " (('disaster',), 158),\n",
              " (('come',), 158),\n",
              " (('body',), 154),\n",
              " (('good',), 153),\n",
              " (('attack',), 152),\n",
              " (('bomb',), 150),\n",
              " (('year',), 147),\n",
              " (('look',), 145),\n",
              " (('day',), 143),\n",
              " (('police',), 142),\n",
              " (('man',), 141),\n",
              " (('time',), 140),\n",
              " (('know',), 138),\n",
              " (('home',), 138),\n",
              " (('family',), 129),\n",
              " (('think',), 129),\n",
              " (('storm',), 128)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ba2wJUZvn_9J",
        "outputId": "cf8f5590-f65b-45af-b6f7-10d7f271a16f"
      },
      "source": [
        "Bigrams = ngrams(corpus, 2)\n",
        "\n",
        "# get the frequency of each bigram in our corpus\n",
        "esBigramFreq = collections.Counter(Bigrams)\n",
        "\n",
        "# what are the ten most popular ngrams in this Spanish corpus?\n",
        "esBigramFreq.most_common(30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('suicide', 'bomber'), 60),\n",
              " (('body', 'bag'), 58),\n",
              " (('look', 'like'), 55),\n",
              " (('burn', 'building'), 54),\n",
              " (('northern', 'california'), 42),\n",
              " (('like', '@youtube'), 41),\n",
              " (('@youtube', 'video'), 41),\n",
              " (('cross', 'body'), 40),\n",
              " (('oil', 'spill'), 39),\n",
              " (('california', 'wildfire'), 36),\n",
              " (('suicide', 'bombing'), 33),\n",
              " (('mass', 'murder'), 33),\n",
              " (('70', 'year'), 31),\n",
              " (('mass', 'murderer'), 31),\n",
              " (('bomber', 'detonate'), 31),\n",
              " (('natural', 'disaster'), 31),\n",
              " (('prebreak', 'good'), 30),\n",
              " (('feel', 'like'), 30),\n",
              " (('heat', 'wave'), 30),\n",
              " (('home', 'raze'), 29),\n",
              " (('confirm', 'mh370'), 29),\n",
              " (('detonate', 'bomb'), 29),\n",
              " (('late', 'home'), 28),\n",
              " (('raze', 'northern'), 28),\n",
              " (('16yr', 'old'), 28),\n",
              " (('pkk', 'suicide'), 28),\n",
              " (('severe', 'thunderstorm'), 27),\n",
              " (('obama', 'declare'), 27),\n",
              " (('rescuer', 'search'), 27),\n",
              " (('old', 'pkk'), 27)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybrjHz9FnP_4"
      },
      "source": [
        "import gensim.downloader\n",
        "import multiprocessing\n",
        "\n",
        "def eval_pipeline(df_train, df_test, tokenize_function, target_col='keyword', feature_engineering='tfidf'):\n",
        "  ''' Eval pipeline '''\n",
        "\n",
        "  # Make copies to avoid any changes in the main datasets\n",
        "  train = df_train.copy()\n",
        "  test = df_test.copy()\n",
        "\n",
        "  # Define the model\n",
        "  model = LogisticRegression(solver='lbfgs', #LogisticRegressionCV\n",
        "                               #cv=2, \n",
        "                               max_iter=1000, \n",
        "                               random_state=50)\n",
        "\n",
        "  # Define embeddings and model\n",
        "  if feature_engineering == 'tfidf':\n",
        "    feature_vector = TfidfVectorizer(tokenizer=tokenize_function)\n",
        "    model = Pipeline([('vectorizer', feature_vector), \n",
        "                   ('classifier', model)])\n",
        "  \n",
        "  elif feature_engineering == 'w2v':\n",
        "\n",
        "    # Function that uses w2v embeddings and compute the mean value of all the of w2v vectors of all the words\n",
        "    def document_vector(doc):\n",
        "      \"\"\"Create document vectors by averaging word vectors. Remove out-of-vocabulary words.\"\"\"\n",
        "      tokens = tokenize_function(doc)\n",
        "      doc = [word for word in tokens if word in feature_vector.wv.vocab]\n",
        "      return np.mean(feature_vector[doc], axis=0)\n",
        "\n",
        "    # Create corpus for w2v\n",
        "    sentences = []\n",
        "    for text in [list(train[target_col]), list(test[target_col])]:\n",
        "      for string in text:\n",
        "        sentences.append(tokenize_function(string))\n",
        "\n",
        "    # Create w2v embeddings\n",
        "    feature_vector = gensim.models.Word2Vec(sentences, \n",
        "                                            size=100, \n",
        "                                            window=10, \n",
        "                                            min_count=1, \n",
        "                                            iter=20,\n",
        "                                            seed=50,\n",
        "                                            workers=multiprocessing.cpu_count()\n",
        "                                            )\n",
        "\n",
        "    train[target_col] = train[target_col].apply(document_vector)\n",
        "    test[target_col] = test[target_col].apply(document_vector)\n",
        "  \n",
        "  # Split dataset\n",
        "  df_train_eval, df_validation = train_test_split(train, test_size=0.2, random_state = 50)\n",
        "\n",
        "  y_train_eval = df_train_eval[[\"target\"]]\n",
        "  y_validation = df_validation[[\"target\"]]\n",
        "\n",
        "  X1_train_eval = list(df_train_eval[target_col])\n",
        "  X1_validation = list(df_validation[target_col])\n",
        "\n",
        "  # Fit data\n",
        "  model.fit(X1_train_eval, y_train_eval)\n",
        "\n",
        "  # Eval data\n",
        "  train_accuracy_score = accuracy_score(y_train_eval, model.predict(X1_train_eval))\n",
        "  print(f\"The training accuracy is : {train_accuracy_score}\")\n",
        "\n",
        "  validation_accuracy_score = accuracy_score(y_validation, model.predict(X1_validation))\n",
        "  print(f\"The validation accuracy is : {validation_accuracy_score}\")\n",
        "\n",
        "  # Retrain on the full dataset\n",
        "  y_train = train[[\"target\"]]\n",
        "  X1_train = list(train[target_col])\n",
        "  X1_test = list(test[target_col])\n",
        "\n",
        "  # Create submission file\n",
        "  model.fit(X1_train, y_train)\n",
        "  submission = pd.DataFrame(model.predict(X1_test), columns=[\"target\"])\n",
        "  submission.to_csv(f'submission_{target_col}_{feature_engineering}_model_LogisticRegressionCV2_test2.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joU8n6qdq7Xi"
      },
      "source": [
        "###With \"Keyword\" feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mF8QhZD6OhAk",
        "outputId": "88f5ef91-88cf-4feb-bfc3-a7555c0c5413"
      },
      "source": [
        "eval_pipeline(df_train, df_test, tokenize_function, target_col='keyword', feature_engineering='tfidf')\n",
        "#eval_pipeline(df_train, df_test, tokenize_function, target_col='keyword', feature_engineering='w2v')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The training accuracy is : 0.7436244204018547\n",
            "The validation accuracy is : 0.6988416988416989\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYFI4-yyb_90"
      },
      "source": [
        "###With \"Text\" Feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPH9eT80UI5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ab94f6b-c146-4775-a630-672ea4e2d42c"
      },
      "source": [
        "eval_pipeline(df_train, df_test, tokenize_function, target_col='text', feature_engineering='tfidf')\n",
        "#eval_pipeline(df_train, df_test, tokenize_function, target_col='text', feature_engineering='w2v')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The training accuracy is : 0.8858191653786708\n",
            "The validation accuracy is : 0.803088803088803\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8y1jbGygN5w"
      },
      "source": [
        "###With \"Text\" and \"Keyword\" features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pj6aSW9WUN2i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c090bf0-ec66-4862-8ec2-1b6eb717ebf4"
      },
      "source": [
        "eval_pipeline(df_train, df_test, tokenize_function, target_col='keyword_text', feature_engineering='tfidf')\n",
        "#eval_pipeline(df_train, df_test, tokenize_function, target_col='keyword_text', feature_engineering='w2v')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The training accuracy is : 0.9290958268933539\n",
            "The validation accuracy is : 0.7814671814671814\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZxmd_XBKkPy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
