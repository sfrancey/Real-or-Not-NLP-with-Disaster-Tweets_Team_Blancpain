{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copie de New_Notebook_Blancpain.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sfrancey/Real-or-Not-NLP-with-Disaster-Tweets_Team_Blancpain/blob/main/Copie_de_New_Notebook_Blancpain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEA9bJP9nXuD"
      },
      "source": [
        "#DMML TEAM BLANCPAIN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_joXxs5cncOK"
      },
      "source": [
        "##Import some librairies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLs2TYMXnkrn"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import collections  as mc\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import pandas as pd \n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "sns.set_style(\"white\")\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import datasets\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "from sklearn.base import TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "import string\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from spacy.lang.en import English\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from sklearn.base import TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "import spacy\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "%matplotlib inline\n",
        "sns.set_style(\"dark\")\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqF5jThxnsXJ"
      },
      "source": [
        "##Data Importation##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BM5SUhspoHyz"
      },
      "source": [
        "df_train = pd.read_csv('https://raw.githubusercontent.com/sfrancey/Real-or-Not-NLP-with-Disaster-Tweets_Team_Blancpain/main/Data/training_data.csv')\n",
        "\n",
        "df_test = pd.read_csv('https://raw.githubusercontent.com/sfrancey/Real-or-Not-NLP-with-Disaster-Tweets_Team_Blancpain/main/Data/test_data.csv')\n",
        "\n",
        "df_sample = pd.read_csv('https://raw.githubusercontent.com/sfrancey/Real-or-Not-NLP-with-Disaster-Tweets_Team_Blancpain/main/Data/sample_submission.csv')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYUyrcLcoeC9"
      },
      "source": [
        "##Data visualization##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvR6D2f3or1z"
      },
      "source": [
        "###Training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "RJ4MVIQAok60",
        "outputId": "b8d5c741-b660-45d3-f5d0-dc58a07ec540"
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3738</td>\n",
              "      <td>destroyed</td>\n",
              "      <td>USA</td>\n",
              "      <td>Black Eye 9: A space battle occurred at Star O...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>853</td>\n",
              "      <td>bioterror</td>\n",
              "      <td>NaN</td>\n",
              "      <td>#world FedEx no longer to transport bioterror ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10540</td>\n",
              "      <td>windstorm</td>\n",
              "      <td>Palm Beach County, FL</td>\n",
              "      <td>Reality Training: Train falls off elevated tra...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5988</td>\n",
              "      <td>hazardous</td>\n",
              "      <td>USA</td>\n",
              "      <td>#Taiwan Grace: expect that large rocks trees m...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6328</td>\n",
              "      <td>hostage</td>\n",
              "      <td>Australia</td>\n",
              "      <td>New ISIS Video: ISIS Threatens to Behead Croat...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id    keyword  ...                                               text target\n",
              "0   3738  destroyed  ...  Black Eye 9: A space battle occurred at Star O...      0\n",
              "1    853  bioterror  ...  #world FedEx no longer to transport bioterror ...      0\n",
              "2  10540  windstorm  ...  Reality Training: Train falls off elevated tra...      1\n",
              "3   5988  hazardous  ...  #Taiwan Grace: expect that large rocks trees m...      1\n",
              "4   6328    hostage  ...  New ISIS Video: ISIS Threatens to Behead Croat...      1\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qURApsPqoncX",
        "outputId": "a56a57c5-304b-448e-b3de-5303913fdd8c"
      },
      "source": [
        "print(\"There are {0} rows and {1} columns in the train dataset.\".format(df_train.shape[0],df_train.shape[1]))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 6471 rows and 5 columns in the train dataset.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2OXEAZ_ou10"
      },
      "source": [
        "###Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "1MchgznVoqL1",
        "outputId": "4d72d3f1-6550-4a17-de08-9ed3698c6f21"
      },
      "source": [
        "df_test.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9972</td>\n",
              "      <td>tsunami</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Crptotech tsunami and banks.\\n http://t.co/KHz...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9865</td>\n",
              "      <td>traumatised</td>\n",
              "      <td>Portsmouth, UK</td>\n",
              "      <td>I'm that traumatised that I can't even spell p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1937</td>\n",
              "      <td>burning%20buildings</td>\n",
              "      <td>NaN</td>\n",
              "      <td>@foxnewsvideo @AIIAmericanGirI @ANHQDC So ... ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3560</td>\n",
              "      <td>desolate</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Me watching Law &amp;amp; Order (IB: @sauldale305)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2731</td>\n",
              "      <td>crushed</td>\n",
              "      <td>bahstun/porta reeko</td>\n",
              "      <td>Papi absolutely crushed that ball</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     id  ...                                               text\n",
              "0  9972  ...  Crptotech tsunami and banks.\\n http://t.co/KHz...\n",
              "1  9865  ...  I'm that traumatised that I can't even spell p...\n",
              "2  1937  ...  @foxnewsvideo @AIIAmericanGirI @ANHQDC So ... ...\n",
              "3  3560  ...  Me watching Law &amp; Order (IB: @sauldale305)...\n",
              "4  2731  ...                  Papi absolutely crushed that ball\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LxZk2puo9qt",
        "outputId": "a5296f53-adaa-4889-8815-b92ee127d6dc"
      },
      "source": [
        "print(\"There are {0} rows and {1} columns in the test dataset.\".format(df_test.shape[0],df_test.shape[1]))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1142 rows and 4 columns in the test dataset.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "g1S1HLPzp6qU",
        "outputId": "0e104356-e406-417e-f96c-68db319e1df9"
      },
      "source": [
        "df_sample.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   target\n",
              "0       0\n",
              "1       0\n",
              "2       0\n",
              "3       0\n",
              "4       0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OjBhP9fEHh-"
      },
      "source": [
        "###Drop Null values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMdir8zQELnj"
      },
      "source": [
        "df_train.fillna(\"Unknown\", inplace = True)\n",
        "df_test.fillna(\"Unknown\", inplace = True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPH63UivpBx7"
      },
      "source": [
        "###Base Rate "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxTO-v2jpKxb",
        "outputId": "764b1869-272a-4b84-8030-8095ee411833"
      },
      "source": [
        "base_rate = max(len(df_train[df_train[\"target\"] == 0]) / len(df_train), len(df_train[df_train[\"target\"] == 1]) / len(df_train))\n",
        "print(\"The base rate for this problem is :\", base_rate)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The base rate for this problem is : 0.5719363313243703\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Fvm4vyypZ4f"
      },
      "source": [
        "##Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0L8vhbjp9oZ"
      },
      "source": [
        "df_train[\"keyword_text\"] = df_train['keyword'] + ' ' + df_train['text'] \n",
        "df_train[\"keyword_text\"] = df_train[\"keyword_text\"].astype(str)\n",
        "df_test[\"keyword_text\"] = df_test['keyword'] + ' ' + df_test['text'] \n",
        "df_test[\"keyword_text\"] = df_test[\"keyword_text\"].astype(str)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "id": "-4JtEGnsp_4v",
        "outputId": "e9eefda8-dcce-4f08-b04d-467fabacb844"
      },
      "source": [
        "df_train"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "      <th>keyword_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3738</td>\n",
              "      <td>destroyed</td>\n",
              "      <td>USA</td>\n",
              "      <td>Black Eye 9: A space battle occurred at Star O...</td>\n",
              "      <td>0</td>\n",
              "      <td>destroyed Black Eye 9: A space battle occurred...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>853</td>\n",
              "      <td>bioterror</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>#world FedEx no longer to transport bioterror ...</td>\n",
              "      <td>0</td>\n",
              "      <td>bioterror #world FedEx no longer to transport ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10540</td>\n",
              "      <td>windstorm</td>\n",
              "      <td>Palm Beach County, FL</td>\n",
              "      <td>Reality Training: Train falls off elevated tra...</td>\n",
              "      <td>1</td>\n",
              "      <td>windstorm Reality Training: Train falls off el...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5988</td>\n",
              "      <td>hazardous</td>\n",
              "      <td>USA</td>\n",
              "      <td>#Taiwan Grace: expect that large rocks trees m...</td>\n",
              "      <td>1</td>\n",
              "      <td>hazardous #Taiwan Grace: expect that large roc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6328</td>\n",
              "      <td>hostage</td>\n",
              "      <td>Australia</td>\n",
              "      <td>New ISIS Video: ISIS Threatens to Behead Croat...</td>\n",
              "      <td>1</td>\n",
              "      <td>hostage New ISIS Video: ISIS Threatens to Behe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6466</th>\n",
              "      <td>4377</td>\n",
              "      <td>earthquake</td>\n",
              "      <td>ARGENTINA</td>\n",
              "      <td>#Earthquake #Sismo M 1.9 - 15km E of Anchorage...</td>\n",
              "      <td>1</td>\n",
              "      <td>earthquake #Earthquake #Sismo M 1.9 - 15km E o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6467</th>\n",
              "      <td>3408</td>\n",
              "      <td>derail</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>@EmiiliexIrwin Totally agree.She is 23 and kno...</td>\n",
              "      <td>0</td>\n",
              "      <td>derail @EmiiliexIrwin Totally agree.She is 23 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6468</th>\n",
              "      <td>9794</td>\n",
              "      <td>trapped</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>Hollywood Movie About Trapped Miners Released ...</td>\n",
              "      <td>1</td>\n",
              "      <td>trapped Hollywood Movie About Trapped Miners R...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6469</th>\n",
              "      <td>10344</td>\n",
              "      <td>weapons</td>\n",
              "      <td>Beirut/Toronto</td>\n",
              "      <td>Friendly reminder that the only country to eve...</td>\n",
              "      <td>1</td>\n",
              "      <td>weapons Friendly reminder that the only countr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6470</th>\n",
              "      <td>1779</td>\n",
              "      <td>buildings%20on%20fire</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>Buildings are on fire and they have time for a...</td>\n",
              "      <td>1</td>\n",
              "      <td>buildings%20on%20fire Buildings are on fire an...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6471 rows Ã— 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         id  ...                                       keyword_text\n",
              "0      3738  ...  destroyed Black Eye 9: A space battle occurred...\n",
              "1       853  ...  bioterror #world FedEx no longer to transport ...\n",
              "2     10540  ...  windstorm Reality Training: Train falls off el...\n",
              "3      5988  ...  hazardous #Taiwan Grace: expect that large roc...\n",
              "4      6328  ...  hostage New ISIS Video: ISIS Threatens to Behe...\n",
              "...     ...  ...                                                ...\n",
              "6466   4377  ...  earthquake #Earthquake #Sismo M 1.9 - 15km E o...\n",
              "6467   3408  ...  derail @EmiiliexIrwin Totally agree.She is 23 ...\n",
              "6468   9794  ...  trapped Hollywood Movie About Trapped Miners R...\n",
              "6469  10344  ...  weapons Friendly reminder that the only countr...\n",
              "6470   1779  ...  buildings%20on%20fire Buildings are on fire an...\n",
              "\n",
              "[6471 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4fqQMMksxTy"
      },
      "source": [
        "# Create validation set to test our best model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDKFEoQ8qRpE"
      },
      "source": [
        "df_train_eval, df_validation = train_test_split(df_train, test_size=0.2, random_state = 50)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txFEHAM_Kl82"
      },
      "source": [
        "y_train_eval = df_train_eval[[\"target\"]]\n",
        "y_validation = df_validation[[\"target\"]]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joU8n6qdq7Xi"
      },
      "source": [
        "###With \"Keyword\" feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njb8fBH-QWXy"
      },
      "source": [
        "X1_train_eval = df_train_eval['keyword']\n",
        "X1_validation = df_validation['keyword']"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsqY4mVwXf0E"
      },
      "source": [
        "# Load English language model\n",
        "sp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Create a list of punctuation marks\n",
        "punctuations = string.punctuation\n",
        "\n",
        "# Create a list of stopwords\n",
        "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
        "\n",
        "def tokenize_function(text):\n",
        "    # Create token object, which is used to create documents with linguistic annotations.\n",
        "    mytokens = sp(text)\n",
        "\n",
        "    # Lemmatize each token and convert each token into lowercase\n",
        "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
        "\n",
        "    ## Remove stop words and punctuation\n",
        "    #mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
        "\n",
        "    return mytokens"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TD8yRFeHtnAc",
        "outputId": "0bcb4313-7456-4265-dd09-0cae1ac527de"
      },
      "source": [
        "tokenize_function(\"j'aime le machine learning\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"j'aime\", 'le', 'machine', 'learning']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mKJ6x1OXpXi"
      },
      "source": [
        "tfidf_feature_vector = TfidfVectorizer(tokenizer=tokenize_function)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUTzSz-1Tqb2"
      },
      "source": [
        "l_reg_CV = LogisticRegressionCV(solver='lbfgs', cv=2, max_iter=1000, random_state=50)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Psz5CLWcWbMe"
      },
      "source": [
        "pipe = Pipeline([('vectorizer', tfidf_feature_vector), \n",
        "                 ('classifier', l_reg_CV)])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkcNZNFYYJvh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "839b339c-d616-4381-82ea-5eb9fd2564a9"
      },
      "source": [
        "pipe.fit(X1_train_eval, y_train_eval)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('vectorizer',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_patt...\n",
              "                                 tokenizer=<function tokenize_function at 0x7f3306468950>,\n",
              "                                 use_idf=True, vocabulary=None)),\n",
              "                ('classifier',\n",
              "                 LogisticRegressionCV(Cs=10, class_weight=None, cv=2,\n",
              "                                      dual=False, fit_intercept=True,\n",
              "                                      intercept_scaling=1.0, l1_ratios=None,\n",
              "                                      max_iter=1000, multi_class='auto',\n",
              "                                      n_jobs=None, penalty='l2',\n",
              "                                      random_state=50, refit=True, scoring=None,\n",
              "                                      solver='lbfgs', tol=0.0001, verbose=0))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsoZ39YKb1i7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43646050-c555-49fb-c64a-e904758a8777"
      },
      "source": [
        "train_accuracy_score = accuracy_score(y_train_eval, pipe.predict(X1_train_eval))\n",
        "print(\"The training accuracy is\", train_accuracy_score)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The training accuracy is 0.7436244204018547\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNFiQQlcw94z",
        "outputId": "bc9ae9d6-cf31-4c09-fc4d-5be6329a280d"
      },
      "source": [
        "score_test = accuracy_score(y_validation, pipe.predict(X1_validation))\n",
        "print(\"The test accuracy is\", score_test)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The test accuracy is 0.6988416988416989\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snQBqAG_048n"
      },
      "source": [
        "import gensim.downloader\n",
        "\n",
        "def eval_pipeline(df_train, df_test, tokenize_function, target_col='keyword', feature_engineering='tfidf'):\n",
        "  ''' Eval pipeline '''\n",
        "\n",
        "  train = df_train.copy()\n",
        "  test = df_test.copy()\n",
        "\n",
        "  model = LogisticRegressionCV(solver='lbfgs', cv=2, max_iter=1000, random_state=50)\n",
        "  \n",
        "  def document_vector(doc):\n",
        "    \"\"\"Create document vectors by averaging word vectors. Remove out-of-vocabulary words.\"\"\"\n",
        "    tokens = tokenize_function(doc)\n",
        "    doc = [word for word in tokens if word in feature_vector.wv.vocab]\n",
        "    return np.mean(feature_vector[doc], axis=0)\n",
        "\n",
        "  # Define embeddings and model\n",
        "  if feature_engineering == 'tfidf':\n",
        "    feature_vector = TfidfVectorizer(tokenizer=tokenize_function)\n",
        "    model = Pipeline([('vectorizer', feature_vector), \n",
        "                   ('classifier', model)])\n",
        "  \n",
        "  elif feature_engineering == 'w2v':\n",
        "    # Create corpus for w2v\n",
        "    sentences = []\n",
        "    for text in [list(df_train[target_col]), list(df_test[target_col])]:\n",
        "      for string in text:\n",
        "        sentences.append(tokenize_function(string))\n",
        "\n",
        "    # Create w2v embeddings\n",
        "    feature_vector = gensim.models.Word2Vec(sentences, size=100, window=10, min_count=2, iter=20)\n",
        "    train[target_col] = train[target_col].apply(document_vector)\n",
        "    test[target_col] = test[target_col].apply(document_vector)\n",
        "  \n",
        "  # Split dataset\n",
        "  df_train_eval, df_validation = train_test_split(train, test_size=0.2, random_state = 50)\n",
        "\n",
        "  y_train_eval = df_train_eval[[\"target\"]]\n",
        "  y_validation = df_validation[[\"target\"]]\n",
        "\n",
        "  X1_train_eval = list(df_train_eval[target_col])\n",
        "  X1_validation = list(df_validation[target_col])\n",
        "\n",
        "  # Fit data\n",
        "  model.fit(X1_train_eval, y_train_eval)\n",
        "\n",
        "  # Eval data\n",
        "  train_accuracy_score = accuracy_score(y_train_eval, model.predict(X1_train_eval))\n",
        "  print(\"The training accuracy is\", train_accuracy_score)\n",
        "\n",
        "  score_test = accuracy_score(y_validation, model.predict(X1_validation))\n",
        "  print(\"The test accuracy is\", score_test)\n",
        "\n",
        "  y_train = train[[\"target\"]]\n",
        "  X1_train = list(train[target_col])\n",
        "  X1_test = list(test[target_col])\n",
        "\n",
        "  # Create submission file\n",
        "  model_w2v.fit(X1_train, y_train)\n",
        "  submission = pd.DataFrame(model.predict(X1_test), columns=[\"target\"])\n",
        "  submission.to_csv(f'submission_{target_col}_{feature_engineering}_model_LogisticRegressionCV2_test1.csv', index=False)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mF8QhZD6OhAk",
        "outputId": "075e5c69-409b-40f3-b980-c7a8d8120670"
      },
      "source": [
        "eval_pipeline(df_train, df_test, tokenize_function, target_col='keyword', feature_engineering='w2v')\n",
        "eval_pipeline(df_train, df_test, tokenize_function, target_col='keyword', feature_engineering='tfidf')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The training accuracy is 0.6895285935085008\n",
            "The test accuracy is 0.6486486486486487\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The training accuracy is 0.7436244204018547\n",
            "The test accuracy is 0.6988416988416989\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-ff74d1d416d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0meval_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenize_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'keyword'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_engineering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'w2v'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0meval_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenize_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'keyword'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_engineering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tfidf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-35-ccdde4f1cdb4>\u001b[0m in \u001b[0;36meval_pipeline\u001b[0;34m(df_train, df_test, tokenize_function, target_col, feature_engineering)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m   \u001b[0;31m# Create submission file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m   \u001b[0mmodel_w2v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m   \u001b[0msubmission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m   \u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'submission_{target_col}_{feature_engineering}_model_LogisticRegressionCV2_test1.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   2002\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=np.float64,\n\u001b[1;32m   2003\u001b[0m                          \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2004\u001b[0;31m                          accept_large_sparse=solver != 'liblinear')\n\u001b[0m\u001b[1;32m   2005\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    753\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    756\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    529\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'destroyed'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POI2RandxbNa"
      },
      "source": [
        "# Submission with keywords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2o4YVwkPlnI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "-onFi7UjxaXX",
        "outputId": "56bdb3af-fd56-48c2-f212-1572cb0a1edd"
      },
      "source": [
        "y_train = df_train[[\"target\"]]\n",
        "X1_train = list(df_train['keyword'])\n",
        "X1_test = list(df_test['keyword'])\n",
        "\n",
        "model_w2v.fit(X1_train, y_train)\n",
        "submission = pipe.predict(X1_test)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-c24ad3cb761a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX1_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'keyword'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel_w2v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0msubmission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   2002\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=np.float64,\n\u001b[1;32m   2003\u001b[0m                          \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2004\u001b[0;31m                          accept_large_sparse=solver != 'liblinear')\n\u001b[0m\u001b[1;32m   2005\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    753\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    756\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    529\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'destroyed'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpMvPgLFySMO"
      },
      "source": [
        "submission = pd.DataFrame(submission, columns=[\"target\"])\n",
        "submission.to_csv('submission_keywords_w2v_model_LogisticRegressionCV2_test1.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcWH1gaIO-aL"
      },
      "source": [
        "y_train = df_train[[\"target\"]]\n",
        "X1_train = list(df_train['keyword'])\n",
        "X1_test = list(df_test['keyword'])\n",
        "\n",
        "model_tfidf.fit(X1_train, y_train)\n",
        "submission = pipe.predict(X1_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faIydiC7PBB-"
      },
      "source": [
        "submission = pd.DataFrame(submission, columns=[\"target\"])\n",
        "submission.to_csv('submission_keywords_tfidf_model_LogisticRegressionCV2_test1.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYFI4-yyb_90"
      },
      "source": [
        "###With \"Text\" Feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKv-_8_DcX9J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "9bf088db-bd97-4da0-b2ef-453a4142072b"
      },
      "source": [
        "X2_train_train = df_train_train[\"text\"]\n",
        "X2_train_test = df_train_test[\"text\"]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-182638b01647>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX2_train_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_train_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX2_train_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_train_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_train_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_m7sW07cjzN"
      },
      "source": [
        "sp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def tokenize_function(tweet):\n",
        "    tokens = sp(tweet)\n",
        "    tokens = [ i.lower_ for i in tokens ]\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVBF36Xtc8zG"
      },
      "source": [
        "tfidf_feature_vector = TfidfVectorizer(tokenizer=tokenize_function)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uH93Jwq-dC6-"
      },
      "source": [
        "classifierCV = LogisticRegressionCV(solver='lbfgs', cv=5, max_iter=10000, random_state=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ja-B_oaPdGSH"
      },
      "source": [
        "pipe = Pipeline([('vectorizer', tfidf_feature_vector),\n",
        "                 ('classifier', classifierCV)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TKa3JRjdKDT"
      },
      "source": [
        "pipe.fit(X2_train_train, y_train_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mi_EIqXffO8q"
      },
      "source": [
        "y_pred = pipe.predict(X2_train_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytpzA2jCfjeb"
      },
      "source": [
        "score_test = accuracy_score(y_train_test, y_pred)\n",
        "print(\"The test accuracy is\", score_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYmqVEUzgQwW"
      },
      "source": [
        "train_accuracy_score = accuracy_score(y_train_train, pipe.predict(X2_train_train))\n",
        "print(\"The training accuracy is\", train_accuracy_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPH9eT80UI5c"
      },
      "source": [
        "eval_pipeline(df_train, df_test, tokenize_function, target_col='text', feature_engineering='w2v')\n",
        "eval_pipeline(df_train, df_test, tokenize_function, target_col='text', feature_engineering='tfidf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8y1jbGygN5w"
      },
      "source": [
        "###With \"Text\" and \"Keyword\" features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOkjVopXg4k-"
      },
      "source": [
        "X3_train_train = df_train_train[\"keyword_text\"]\n",
        "X3_train_test = df_train_test[\"keyword_text\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWzkWPothY-G"
      },
      "source": [
        "sp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def tokenize_function(tweet):\n",
        "    tokens = sp(tweet)\n",
        "    tokens = [ i.lower_ for i in tokens ]\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-1rvrzehb-g"
      },
      "source": [
        "tfidf_feature_vector = TfidfVectorizer(tokenizer=tokenize_function)\n",
        "classifierCV = LogisticRegressionCV(solver='lbfgs', cv=5, max_iter=10000, random_state=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gx5XwwtcheyW"
      },
      "source": [
        "pipe = Pipeline([('vectorizer', tfidf_feature_vector),\n",
        "                 ('classifier', classifierCV)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zi5abPYQhkJ1"
      },
      "source": [
        "pipe.fit(X3_train_train, y_train_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdN7r6ZIhpvU"
      },
      "source": [
        "y_pred = pipe.predict(X3_train_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zh0sN-ChhtyF"
      },
      "source": [
        "score_test = accuracy_score(y_train_test, y_pred)\n",
        "print(\"The test accuracy is\", score_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rWViG-rhuck"
      },
      "source": [
        "train_accuracy_score = accuracy_score(y_train_train, pipe.predict(X3_train_train))\n",
        "print(\"The training accuracy is\", train_accuracy_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pj6aSW9WUN2i"
      },
      "source": [
        "eval_pipeline(df_train, df_test, tokenize_function, target_col='keyword_text', feature_engineering='w2v')\n",
        "eval_pipeline(df_train, df_test, tokenize_function, target_col='keyword_text', feature_engineering='tfidf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okIEwrBOUP-U"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}